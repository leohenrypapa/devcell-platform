# Knowledgebase Module

The Knowledgebase (KB) module provides DevCellâ€™s document storage, embedding,
semantic search, and Retrieval-Augmented Generation (RAG) capabilities. It is
designed for environments that require local-first knowledge storage and
offline access.

This document describes the full KB architecture: data model, pipelines,
service logic, embedding workflow, RAG search, frontend UI, and planned future
extensions.

---

# ğŸ¯ Purpose

The Knowledgebase enables teams to:

- store technical documents locally
- embed documents into a vector database (Chroma)
- perform semantic search across stored knowledge
- enrich chat/SITREP queries with contextual retrieval
- support training pipelines and documentation workflows

The KB module is fully local â€” no cloud APIs are required.

---

# ğŸ§± Data Model

Knowledgebase metadata is stored in the `knowledge_docs` table:

```

id INTEGER PRIMARY KEY
filename TEXT
original_name TEXT

source_type TEXT          -- upload, autogenerated (future)
file_path TEXT            -- full local path

embedding_status TEXT     -- pending | embedded | failed

created_at TEXT
updated_at TEXT

```

### Additional storage layers

1. **Filesystem**  
   Raw documents stored under:  
```

backend/app/knowledgebase/docs/

```

2. **Chroma Vector Index**  
Embeddings stored in a persistent ChromaDB collection.

---

# ğŸ§© Backend Architecture

The KB module is implemented in a dedicated folder:

```

backend/app/knowledgebase/
documents.py
embedder.py
indexer.py
query.py
rag.py

```

Each component is responsible for one stage of the pipeline.

---

## ğŸ“ 1. Document Management (`documents.py`)

Responsibilities:

- validate uploaded file types
- assign unique filename
- save raw file to disk
- delete raw file when document is removed
- ensure safe overwrite behavior

Validation prevents:
- empty files  
- unsupported formats  
- path traversal  

---

## ğŸ§  2. Embedding Pipeline (`embedder.py`)

Converts documents into embeddings using the configured LLM endpoint.

Steps:

1. Read file from filesystem  
2. Preprocess text (strip, normalize)  
3. Split into chunks (size determined by model context window)  
4. Send embedding requests to LLM  
5. Return list of embedding vectors  

Errors update `embedding_status = failed`.

---

## ğŸ§± 3. Indexing Layer (Chroma) (`indexer.py`)

Indexing stores each document chunk as:

```

{
"id": chunk_id,
"text": chunk_text,
"embedding": [ ... ],
"metadata": { doc_id, filename, chunk_index }
}

```

Operations:
- add document chunks  
- delete chunks for a document  
- query by vector similarity  

Uses persistent Chroma collection so vectors survive restarts.

---

## ğŸ” 4. Semantic Search (`query.py`)

Implements KB search:

1. Embed the query text  
2. Query Chroma collection  
3. Retrieve top matching chunks  
4. Return structured results:
```

[ { text, score, metadata }, ... ]

```

Used by KB Search page and SITREP generation.

---

## ğŸ¤– 5. RAG Pipeline (`rag.py`)

RAG combines user query + retrieved knowledge + LLM reasoning.

Pipeline:

1. Receive query text  
2. Use semantic search to fetch context blocks  
3. Construct structured RAG prompt:
```

SYSTEM: You are a domain-aware assistant...
CONTEXT: [chunk1, chunk2, ...]
USER: {query}

```
4. LLM returns answer with citations  
5. Return formatted response  

Used in:
- Chat
- SITREP  
- KB search UI (optional)

---

# ğŸ”Œ Routes & Services

### Routes: `routes/knowledge.py`

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/knowledge/docs` | List stored documents |
| POST | `/api/knowledge/docs` | Upload new doc |
| DELETE | `/api/knowledge/docs/{id}` | Delete doc + embeddings |
| GET | `/api/knowledge/search` | Semantic search |
| POST | `/api/knowledge/rag` | Full RAG query |

### Service: `knowledge_service.py`

Implements:

- upload flow  
- embedding pipeline  
- deletion workflow  
- metadata consistency checks  
- RAG assembly  
- LLM interaction for embeddings & RAG  

Safe deletion ensures:
- remove DB metadata  
- remove filesystem file  
- remove Chroma vectors  

Even if one step fails, system stays consistent.

---

# ğŸ–¥ï¸ Frontend Architecture

Frontend KB logic lives in:

```

src/pages/KnowledgePage.tsx
src/components/KnowledgeUpload.tsx
src/components/KnowledgeList.tsx
src/components/KnowledgeSearch.tsx
src/lib/knowledge.ts

````

### KnowledgePage
Provides three primary panels:
- Docs list  
- Upload area  
- Search / RAG  

### KnowledgeUpload
- Accepts `.txt`, `.md`, and other supported text-based formats  
- Uses drag-and-drop or file picker  
- Shows upload progress and embedding status  

### KnowledgeList
- Displays stored docs  
- Shows embedding status badges  
- Delete button (with confirmation)

### KnowledgeSearch
- Search bar  
- Results panel with chunk previews  
- RAG mode toggle (optional)  

### `lib/knowledge.ts`
Implements:
- `listDocs()`
- `uploadDoc()`
- `deleteDoc()`
- `semanticSearch()`
- `ragQuery()`

Handles JWT injection and network errors.

---

# ğŸ”„ Document Lifecycle

```mermaid
flowchart TD
    U[User Upload] --> V[Validate File]
    V --> S[Save to Filesystem]
    S --> E[LLM Embedding]
    E --> I[Chroma Indexing]
    I --> DONE[Available for Search]

    UDEL[User Deletes Doc] --> RM1[Remove DB Metadata]
    RM1 --> RM2[Remove File]
    RM2 --> RM3[Remove Embedding from Chroma]
````

---

# ğŸ” Search Workflow

1. User enters search phrase
2. Query is embedded
3. Chroma finds relevant chunks
4. Results returned with:

   * chunk text
   * score
   * doc filename
   * doc_id

RAG mode additionally:

* builds context
* sends to LLM
* returns answer + â€œSources sectionâ€

---

# ğŸ” Permissions

Current system: **global KB access**.

| Action          | Allowed           |
| --------------- | ----------------- |
| Upload document | Any user          |
| Delete document | Uploader or Admin |
| Search          | Any user          |
| RAG Query       | Any user          |

Future roadmap (see below) includes project-scoped KB areas.

---

# ğŸ“Š Integration With Other Modules

### Dashboard

SITREP uses:

* KB semantic search for extended context

### Chat

Chat requests can optionally use KB context.

### Training

Training roadmap import may store reference materials in KB (manual action).

---

# ğŸ›‘ Error Handling

The KB module is designed to remain consistent even on partial failures:

### Upload failure cases:

* embedding fails â†’ `embedding_status=failed`
* indexing fails â†’ rollback
* file write fails â†’ abort upload

### Deletion failure:

* Chroma deletion fails â†’ retried next startup (future hook)
* file missing â†’ still delete metadata

Service ensures the system does not break even if LLM is unavailable.

---

# ğŸ”® Future Enhancements

Based on roadmap + CHANGELOG:

### **1. Project-scoped Knowledgebase**

Only members of a project can view/modify associated docs.

### **2. Inline Document Viewer**

Preview:

* markdown
* plaintext
* PDF
* code blocks

### **3. Full document versioning**

Store previous versions, diffs, and embed version history.

### **4. Auto-embedding watch folder**

Drop files into a folder â†’ automatically indexed.

### **5. KB Agents**

LLM agents that:

* summarize documents
* classify content
* tag metadata
* auto-generate training tasks from docs

---

# ğŸ“š Related Documents

* LLM Integration â†’ `architecture/llm_integration.md`
* Data Model â†’ `architecture/data_model.md`
* Permissions â†’ `permissions.md`
* Chat Module â†’ `chat.md`
* Dashboard Module â†’ `dashboard.md`
* API Reference â†’ `../api/knowledge_api.md`

---

```
Â© DevCell Platform Documentation â€” GitHub OSS Style
```